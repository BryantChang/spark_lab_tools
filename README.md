 ## 混合负载自动生成脚本
 ### 1、目录结构
 
 ```
 -scripts
     -bin       ###执行脚本（可单独使用）
        -plan.sh            ###脚本执行主流程
        -setup.sh           ###环境切换，平台环境变量设置
        -workload_gen.sh    ###调用脚本生成模块
        -shutdown.sh        ###停止所有提交任务
     -config  ###配置文件
        -experiment_config  ###实验具体参数设置及变动(例如CPU和MEM的变化等)
        -env.sh             ###相关环境变量（包括负载主目录，负载配置文件路径等）
        -plan.conf          ###实验计划配置，后续脚本将按此计划依次执行
        -workload_cpumem.conf ###应用配比文件（根据不同性质进行配置）
    -lib    ###负载执行脚本jar包
    -script_to_run  ###最终生成执行的脚本位置
    -logs   ###生成过程中的行为日志
    -tools  ###其余相关工具
        -linux_tools  ###系统相关工具（多为批量命令相关）
        -spark_tools  ###spark系统相关操作
 ```
 
 ### 2、配置文件使用及配置
 #### 2.1 env.sh
主要作用：设置和负载生成脚本相关的参数如spark目录，tachyon目录本次实验主要使用spark-bench所以需要设置各个应用的可执行文件以及配置文件的路径，以方便脚本生成 ,文件的主要内容如下：

 ```bash
#!/bin/bash

# Configures for generate submit script.

##################
## DON'T MODIFY ##
##################

HADOOP_HOME=/hadoop_dir
SPARK_HOME=/home/zc/sparkdir
TACHYON_HOME=$SPARK_HOME/tachyon-0.5.0
SPARK_BENCH_HOME=/home/zc/sparkdir/spark-bench
DATA_HDFS_BASE=hdfs://centos1:9000/SparkBench
LAB_CONFIG_DIR=/home/zc/sparkdir/workload_gen/config/experiment_config
#################KMeans#######################
kmeans_executable_basedir=KMeans/bin
kmeans_config_basedir=KMeans/conf

#################PageRank#####################
pagerank_executable_basedir=PageRank/bin
pagerank_config_basedir=PageRank/conf

#################Sort##########################
sort_executable_basedir=KMeans/bin
sort_config_basedir=/KMeans/conf

#################Tpcds#########################
tpcds_executable_basedir=SQL/bin
tpcds_config_basedir=SQL/conf
tpcds_input_hdfs0=tpcds_bin_partitioned_textfile_2
tpcds_input_hdfs1=tpcds_bin_partitioned_textfile_8
tpcds_input_hdfs4=tpcds_bin_partitioned_textfile_30

##################LogisticRegression##############
lr_executable_basedir=LogisticRegression/bin
lr_config_basedir=LogisticRegression/conf

#################WordCount######################
wordcount_executable_basedir=KMeans/bin
wordcount_config_basedir=/KMeans/conf
 ```
 ### 2.2 experiment_config
 主要作用：该文件主要用于设置各个实验组的常规实验参数，如CPU，内存等，文件内容及格式如下：
 ```
 ##cpu&memory:
1 1,1g,0.15 3,1g,0.15 46,2g,0.15 46,4g,0.15 46,4g,0.15 92,12g,0.15 69,6g,0.15
 ```
 ### 2.3 plan.conf
 主要作用：该文件是实验的主要计划配置，用户可以将所有需要进行的进行的实验按指定格式写入该文件，脚本将自动根据该计划执行相应的实验，该文件内容如下：
 ```
  #format workloadType_platform_labno1(cpu/mem)_labno2_labno3_predict-step

 cpumem_spark_1_3_3_2
 
 #cpu：代表负载类型，有CPU、IO、MIX。如果是cpu密集型，那么需要去寻找这种类型混合负载中的应用和配比。
 #hadoop：spark、tachyon、smspark
 #1：cpu mem实验组号
 #3：第一个影响因子实验组号
 #3：第二个影响因子实验组号
 #2：预测步长
 ```
 
 ### 2.4 workload_cpumem.conf
 主要作用：该文件的主要作用是对应用进行配比，在总数量一定的应用中可以指定每一个区间应用的种类及个数，脚本将根据这些配置生成最终需要执行的脚本，该文件内容如下：
 ```
 # CPU--MEM Intensive
# format:jobid script_writer jobAmount
0 bryantchang.workload.generator.Tpcds1Script 36
1 bryantchang.workload.generator.Tpcds1Script 3
2 bryantchang.workload.generator.KMeansScript 5
3 bryantchang.workload.generator.PageRankScript 6
4 bryantchang.workload.generator.KMeansScript 2
5 bryantchang.workload.generator.Tpcds1Script 2
6 bryantchang.workload.generator.PageRankScript 6
 ```
 ## 3、 bin目录中各个脚本的使用方法
 
 ### 3.1 plan.sh
 > * 参数  无参数
 > * 预处理 将2中配置文件写全
 > * 使用方法 sh bin/plan.sh
 
 ### 3.2 setup.sh
 > * 参数  
 >> * spark_version
 >> * log_dir
 > * 预处理 将该脚本开始处环境变量配好
 > * 使用方法 sh bin/setup.sh spark log_dir
 
 ### 3.3 workload_gen.sh
 > * 参数  
 >> * workload_source_conf=$1
 >> * dest_shell_name=$2
 >> * env_path=$3
 >> * lib_path=$4
 >> * cpu_mem_no=$5
 >> * faction1=$6
 >> * faction2=$7
 >> * prediction_step=$8
 >> * log_path=$9
 >> * log_dir
 > * 预处理 将该脚本开始处环境变量配好
 > * 使用方法 sh bin/workload_gen.sh workload_source_conf_path dest_path env_path lib_path cpu_mem_no faction1 faction2 prediction_step log_path
 
 ### 3.4 shut_down.sh
 > * 参数  
 >> * running_script  ##脚本自动生成的文件
 > * 预处理 将该脚本开始处环境变量配好
 > * 使用方法 sh bin/shutdown.sh running_script
 
 ## 4、 tools中脚本的使用方法
参见 [Linux系统小工具](https://github.com/BryantChang/linux-and-spark-tools)